import torch
import torch.nn as nn


class Attention(nn.Module):
    def __init__(self, params):
        super(Attention, self).__init__()
        #paramsè¡¨ç¤ºä¸€äº›è¶…å‚æ•°
        self.params = params
        self.hidden = params['decoder']['hidden_size']
        self.attention_dim = params['attention']['attention_dim']
        self.hidden_weight = nn.Linear(self.hidden, self.attention_dim)
        self.attention_conv = nn.Conv2d(1, 512, kernel_size=11, padding=5, bias=False)
        self.attention_weight = nn.Linear(512, self.attention_dim, bias=False)
        self.alpha_convert = nn.Linear(self.attention_dim, 1)

    #cnn_featuresæ˜¯CNNæå–çš„ç‰¹å¾ï¼Œcnn_features_transæ˜¯CNNæå–çš„ç‰¹å¾çš„è½¬ç½®ï¼Œhiddenæ˜¯decoderçš„éšè—çŠ¶æ€ï¼Œalpha_sumæ˜¯ä¸Šä¸€æ­¥çš„attentionæƒé‡ï¼Œimage_maskæ˜¯å›¾åƒçš„mask
    def forward(self, cnn_features, cnn_features_trans, hidden, alpha_sum, image_mask=None):
        query = self.hidden_weight(hidden)
        alpha_sum_trans = self.attention_conv(alpha_sum)
        # permute(0, 2, 3, 1) åï¼Œå¼ é‡çš„ç»´åº¦è¢«é‡æ–°æ’åˆ—ä¸º [ğµ,ğ»,ğ‘Š,ğ¶][B,H,W,C]
        coverage_alpha = self.attention_weight(alpha_sum_trans.permute(0,2,3,1))
        #queryè¡¨ç¤º[B,1,1,attention_dim]ï¼Œalpha_scoreè¡¨ç¤ºæ³¨æ„åŠ›æƒé‡
        alpha_score = torch.tanh(query[:, None, None, :] + coverage_alpha + cnn_features_trans.permute(0,2,3,1))
        #energyæ˜¯æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ‰“åˆ†å‘é‡ï¼Œè¡¡é‡ä¸åŒä½ç½®çš„ç‰¹å¾å¯¹è§£ç å™¨å½“å‰æ—¶åˆ»çš„è¾“å‡ºæœ‰å¤šé‡è¦
        energy = self.alpha_convert(alpha_score)
        #æ”¾ç½®expæ•°å€¼æº¢å‡ºäº†
        energy = energy - energy.max()
        #squeezeä¼šæŠŠä¸º1çš„ç»´åº¦å»æ‰
        energy_exp = torch.exp(energy.squeeze(-1))
        if image_mask is not None:
            energy_exp = energy_exp * image_mask.squeeze(1)
        alpha = energy_exp / (energy_exp.sum(-1).sum(-1)[:,None,None] + 1e-10)
        #alpha_sum: ç´¯ç§¯æ³¨æ„åŠ›åˆ†å¸ƒï¼Œç”¨äºè¦†ç›–æœºåˆ¶ï¼Œé˜²æ­¢å…³æ³¨åŒºåŸŸè¿‡åº¦é›†ä¸­ã€‚
        alpha_sum = alpha[:,None,:,:] + alpha_sum
        #cnn_featuresæ˜¯CNNæå–çš„ç‰¹å¾å›¾å½¢çŠ¶ä¸ºï¼š[B,C,H,W],sum(-1)è¡¨ç¤ºå¯¹æœ€åçš„ä¸¤ä¸ªç»´åº¦è¿›è¡Œæ±‚å’Œï¼Œæœ€åå¾—åˆ°[B,C]
        #å¾—åˆ°ä¸€ä¸ªèšåˆçš„ä¸Šä¸‹æ–‡å‘é‡ context_vector
        context_vector = (alpha[:,None,:,:] * cnn_features).sum(-1).sum(-1)
        return context_vector, alpha, alpha_sum
